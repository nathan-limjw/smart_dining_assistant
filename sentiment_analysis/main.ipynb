{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5524800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yelp_data():\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    ds = load_dataset(\"Johnnyeee/Yelpdata_663\")\n",
    "    df = pd.concat([ds[split].to_pandas() for split in ds.keys()], ignore_index=True)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "    if \"categories\" in df.columns:\n",
    "        df = df[df[\"categories\"].str.contains(\"restaurant\", case=False, na=False)]\n",
    "        print(f\"Filtered dataset to only retain restaurant reviews: {len(df)} records\")\n",
    "\n",
    "    df[\"sentiment\"] = df[\"stars_x\"].apply(\n",
    "        lambda x: 0 if x < 3 else (1 if x == 3 else 2)\n",
    "    )\n",
    "\n",
    "    df = df[[\"text\", \"sentiment\"]].dropna()\n",
    "\n",
    "    df_balanced = pd.concat(\n",
    "        [\n",
    "            df[df[\"sentiment\"] == 0].sample(n=100000, random_state=42),\n",
    "            df[df[\"sentiment\"] == 1].sample(n=100000, random_state=42),\n",
    "            df[df[\"sentiment\"] == 2].sample(n=100000, random_state=42),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train, temp = train_test_split(\n",
    "        df_balanced, test_size=0.3, stratify=df_balanced[\"sentiment\"], random_state=42\n",
    "    )\n",
    "    val, test = train_test_split(\n",
    "        temp, test_size=0.5, stratify=temp[\"sentiment\"], random_state=42\n",
    "    )\n",
    "\n",
    "    train.to_csv(\"data/train.csv\", index=False)\n",
    "    val.to_csv(\"data/val.csv\", index=False)\n",
    "    test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "    print(f\"Saved train ({len(train)}), val ({len(val)}), test ({len(test)})\")\n",
    "\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def load_and_tokenize_data():\n",
    "    print(\"Loading data splits...\")\n",
    "    train_df = pd.read_csv(\"data/train.csv\")\n",
    "    val_df = pd.read_csv(\"data/val.csv\")\n",
    "    test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    train_df = train_df[:20]\n",
    "    val_df = val_df[:20]\n",
    "    test_df = test_df[:20]\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize(input):\n",
    "        return tokenizer(\n",
    "            input[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "        )\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    print(\"Tokenizing all datasets...\")\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    train_dataset = train_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "    val_dataset = val_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "    test_dataset = test_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "\n",
    "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    print(\"Data tokenized!\")\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def run_hyperparameter_tuning():\n",
    "    print(\"Finding the best hyperparameters...\")\n",
    "\n",
    "    configs = [\n",
    "        {\n",
    "            \"name\": \"config_1_default\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_2_higher_lr\",\n",
    "            \"learning_rate\": 3e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_3_lower_lr_more_epochs\",\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 5,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_4_larger_batch\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_5_lower_weight_decay\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = load_and_tokenize_data()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"Current Config: {config['name']}\")\n",
    "        print(f\"Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"Batch Size: {config['batch_size']}\")\n",
    "        print(f\"Epochs: {config['epochs']}\")\n",
    "        print(f\"Weight Decay: {config['weight_decay']}\")\n",
    "\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=3\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"models/{config['name']}\",\n",
    "            num_train_epochs=config[\"epochs\"],\n",
    "            per_device_train_batch_size=config[\"batch_size\"],\n",
    "            per_device_eval_batch_size=32,\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "            warmup_steps=500,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            logging_dir=f\"results/logs/{config['name']}\",\n",
    "            logging_steps=100,\n",
    "            save_total_limit=2,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        print(\"\\nTraining with configs...\")\n",
    "        start_time = datetime.now()\n",
    "        trainer.train()\n",
    "        end_time = datetime.now()\n",
    "        training_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "        print(\"\\nEvaluating on validation dataset...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        result_entry = {\n",
    "            \"config_name\": config[\"name\"],\n",
    "            \"config_number\": i + 1,\n",
    "            \"learning_rate\": config[\"learning_rate\"],\n",
    "            \"batch_size\": config[\"batch_size\"],\n",
    "            \"epochs\": config[\"epochs\"],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "            \"val_accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"val_precision\": eval_results[\"eval_precision\"],\n",
    "            \"val_recall\": eval_results[\"eval_recall\"],\n",
    "            \"val_f1\": eval_results[\"eval_f1\"],\n",
    "            \"val_loss\": eval_results[\"eval_loss\"],\n",
    "            \"training_time_minutes\": training_time,\n",
    "        }\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "        print(\"\\nTraining Results:\")\n",
    "        print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "        print(f\"Time: {training_time:.1f} mins\")\n",
    "\n",
    "    os.makedirs(\"results/metrics\", exist_ok=True)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(\"val_accuracy\", ascending=False)\n",
    "    results_df.to_csv(\"results/metrics/hyperparam_tuning_results.csv\", index=False)\n",
    "\n",
    "    print(\"Best configuration for Model!\")\n",
    "    best_config = results_df.iloc[0]\n",
    "    print(f\"Config: {best_config['config_name']}\")\n",
    "    print(f\"Accuracy: {best_config['val_accuracy']:.4f}\")\n",
    "\n",
    "    best_config_dict = {\n",
    "        \"config_name\": best_config[\"config_name\"],\n",
    "        \"learning_rate\": float(best_config[\"learning_rate\"]),\n",
    "        \"batch_size\": int(best_config[\"batch_size\"]),\n",
    "        \"epochs\": int(best_config[\"epochs\"]),\n",
    "        \"weight_decay\": float(best_config[\"weight_decay\"]),\n",
    "        \"val_accuracy\": float(best_config[\"val_accuracy\"]),\n",
    "        \"val_f1\": float(best_config[\"val_f1\"]),\n",
    "    }\n",
    "\n",
    "    with open(\"results/metrics/best_config.json\", \"w\") as f:\n",
    "        json.dump(best_config_dict, f, indent=4)\n",
    "\n",
    "    source_dir = f\"models/{best_config['config_name']}\"\n",
    "    dest_dir = \"models/sentiment_model\"\n",
    "\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "\n",
    "    shutil.copytree(source_dir, dest_dir)\n",
    "\n",
    "    print(\"Best model has been saved under: models/sentiment_model\")\n",
    "    print(\n",
    "        \"Results have been saved under 'results/metrics/hyperparam_tuning_results.csv'\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2dd878",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = load_yelp_data()\n",
    "\n",
    "run_hyperparameter_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
