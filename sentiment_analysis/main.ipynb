{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/nathan-limjw/smart_dining_assistant.git\n",
    "\n",
    "# Change directory to the root of the cloned repository\n",
    "%cd smart_dining_assistant/sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5524800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathan/Downloads/DSA4213/Assignments/smart_dining_assistant/sentiment_analysis/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, Trainer, TrainingArguments, pipeline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading dataset...\")\n",
    "    ds = load_dataset(\"Johnnyeee/Yelpdata_663\")\n",
    "\n",
    "    train_df = ds[\"train\"].to_pandas()\n",
    "    test_df = ds[\"test\"].to_pandas()\n",
    "\n",
    "    train_df = train_df[train_df[\"categories\"].str.contains(\"restaurant\", case=False, na=False)]\n",
    "    test_df = test_df[test_df[\"categories\"].str.contains(\"restaurant\", case=False, na=False)]\n",
    "    print(f\"Filtered dataset to only retain restaurant reviews: train = {len(train_df)}, test = {len(test_df)} \")\n",
    "\n",
    "    train_df[\"sentiment\"] = train_df[\"stars_x\"].apply(\n",
    "        lambda x: 0 if x < 3 else (1 if x == 3 else 2)\n",
    "    )\n",
    "    test_df[\"sentiment\"] = test_df[\"stars_x\"].apply(\n",
    "        lambda x: 0 if x < 3 else (1 if x == 3 else 2)\n",
    "    )\n",
    "\n",
    "    train_df = train_df[[\"text\", \"sentiment\"]].dropna()\n",
    "    test_df = test_df[[\"text\", \"sentiment\"]].dropna()\n",
    "\n",
    "    min_train_samples = train_df[\"sentiment\"].value_counts().min()\n",
    "    samples_per_class = min(100000, min_train_samples)\n",
    "\n",
    "    print(f\"Sampling {samples_per_class} from each class\")\n",
    "\n",
    "    train_balanced = pd.concat(\n",
    "        [\n",
    "            train_df[train_df[\"sentiment\"] == 0].sample(n=samples_per_class, random_state=42),\n",
    "            train_df[train_df[\"sentiment\"] == 1].sample(n=samples_per_class, random_state=42),\n",
    "            train_df[train_df[\"sentiment\"] == 2].sample(n=samples_per_class, random_state=42),\n",
    "        ]\n",
    "    )\n",
    "    train_final, val = train_test_split(\n",
    "        train_balanced, test_size=0.2, stratify=train_balanced[\"sentiment\"], random_state=42\n",
    "    )\n",
    "\n",
    "    min_train_samples = test_df[\"sentiment\"].value_counts().min()\n",
    "    test_samples_per_class = min(15000, min_train_samples)\n",
    "\n",
    "    test_balanced = pd.concat(\n",
    "        [\n",
    "            test_df[test_df[\"sentiment\"] == 0].sample(n=test_samples_per_class, random_state=42),\n",
    "            test_df[test_df[\"sentiment\"] == 1].sample(n=test_samples_per_class, random_state=42),\n",
    "            test_df[test_df[\"sentiment\"] == 2].sample(n=test_samples_per_class, random_state=42),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_final.to_csv(\"data/train.csv\", index=False)\n",
    "    val.to_csv(\"data/val.csv\", index=False)\n",
    "    test_balanced.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "    print(\"Data saved to /data\")\n",
    "\n",
    "    return train_final, val, test_balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48411b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def load_and_tokenize_data():\n",
    "    print(\"Loading data splits...\")\n",
    "    train_df = pd.read_csv(\"data/train.csv\")\n",
    "    val_df = pd.read_csv(\"data/val.csv\")\n",
    "    test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    train_df = train_df[:20]\n",
    "    val_df = val_df[:20]\n",
    "    test_df = test_df[:20]\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize(input):\n",
    "        return tokenizer(\n",
    "            input[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "        )\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    print(\"Tokenizing all datasets...\")\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    train_dataset = train_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "    val_dataset = val_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "    test_dataset = test_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "\n",
    "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    print(\"Data tokenized!\")\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def run_hyperparameter_tuning():\n",
    "    print(\"Finding the best hyperparameters...\")\n",
    "\n",
    "    configs = [\n",
    "        {\n",
    "            \"name\": \"config_1_default\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_2_higher_lr\",\n",
    "            \"learning_rate\": 3e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_3_lower_lr_more_epochs\",\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 5,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_4_larger_batch\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"config_5_lower_weight_decay\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 3,\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = load_and_tokenize_data()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"Current Config: {config['name']}\")\n",
    "        print(f\"Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"Batch Size: {config['batch_size']}\")\n",
    "        print(f\"Epochs: {config['epochs']}\")\n",
    "        print(f\"Weight Decay: {config['weight_decay']}\")\n",
    "\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=3\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"models/{config['name']}\",\n",
    "            num_train_epochs=config[\"epochs\"],\n",
    "            per_device_train_batch_size=config[\"batch_size\"],\n",
    "            per_device_eval_batch_size=32,\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "            warmup_steps=500,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            logging_dir=f\"results/logs/{config['name']}\",\n",
    "            logging_steps=100,\n",
    "            save_total_limit=2,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        print(\"\\nTraining with configs...\")\n",
    "        start_time = datetime.now()\n",
    "        trainer.train()\n",
    "        trainer.save_model(f\"models/{config['name']}\") #new\n",
    "\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        tokenizer.save_pretrained(f\"models/{config[\"name\"]}\")\n",
    "\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        training_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "        print(\"\\nEvaluating on validation dataset...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        result_entry = {\n",
    "            \"config_name\": config[\"name\"],\n",
    "            \"config_number\": i + 1,\n",
    "            \"learning_rate\": config[\"learning_rate\"],\n",
    "            \"batch_size\": config[\"batch_size\"],\n",
    "            \"epochs\": config[\"epochs\"],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "            \"val_accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"val_precision\": eval_results[\"eval_precision\"],\n",
    "            \"val_recall\": eval_results[\"eval_recall\"],\n",
    "            \"val_f1\": eval_results[\"eval_f1\"],\n",
    "            \"val_loss\": eval_results[\"eval_loss\"],\n",
    "            \"training_time_minutes\": training_time,\n",
    "        }\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "        print(\"\\nTraining Results:\")\n",
    "        print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "        print(f\"Time: {training_time:.1f} mins\")\n",
    "\n",
    "    os.makedirs(\"results/metrics\", exist_ok=True)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(\"val_accuracy\", ascending=False)\n",
    "    results_df.to_csv(\"results/metrics/hyperparam_tuning_results.csv\", index=False)\n",
    "\n",
    "    print(\"Best configuration for Model!\")\n",
    "    best_config = results_df.iloc[0]\n",
    "    print(f\"Config: {best_config['config_name']}\")\n",
    "    print(f\"Accuracy: {best_config['val_accuracy']:.4f}\")\n",
    "\n",
    "    best_config_dict = {\n",
    "        \"config_name\": best_config[\"config_name\"],\n",
    "        \"learning_rate\": float(best_config[\"learning_rate\"]),\n",
    "        \"batch_size\": int(best_config[\"batch_size\"]),\n",
    "        \"epochs\": int(best_config[\"epochs\"]),\n",
    "        \"weight_decay\": float(best_config[\"weight_decay\"]),\n",
    "        \"val_accuracy\": float(best_config[\"val_accuracy\"]),\n",
    "        \"val_f1\": float(best_config[\"val_f1\"]),\n",
    "    }\n",
    "\n",
    "    with open(\"results/metrics/best_config.json\", \"w\") as f:\n",
    "        json.dump(best_config_dict, f, indent=4)\n",
    "\n",
    "    source_dir = f\"models/{best_config['config_name']}\"\n",
    "    dest_dir = \"models/sentiment_model\"\n",
    "\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "\n",
    "    shutil.copytree(source_dir, dest_dir)\n",
    "\n",
    "    print(\"Best model has been saved under: models/sentiment_model\")\n",
    "    print(\n",
    "        \"Results have been saved under 'results/metrics/hyperparam_tuning_results.csv'\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48665d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true, y_pred, output_path=\"results/figures/confusion_matrix.png\"\n",
    "):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "        yticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix for Sentiment Classification\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return f\"Confusion matrix saved to {output_path}\"\n",
    "\n",
    "\n",
    "def evaluate_model(model_path=\"models/sentiment_model\", test_path=\"data/test.csv\"):\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model_path,\n",
    "        tokenizer=model_path,\n",
    "        return_all_scores=True,\n",
    "    )\n",
    "\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    print(\"\\nMaking predictions on test data...\")\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "\n",
    "    for text in test_df[\"text\"]:\n",
    "        result = classifier(text[:512])[0]\n",
    "        best = max(result, key=lambda x: x[\"score\"])\n",
    "\n",
    "        label_num = int(best[\"label\"].split(\"_\")[1])\n",
    "        predictions.append(label_num)\n",
    "        confidences.append(best[\"score\"])\n",
    "\n",
    "    test_df[\"predicted\"] = predictions\n",
    "    test_df[\"confidence\"] = confidences\n",
    "\n",
    "    y_true = test_df[\"sentiment\"]\n",
    "    y_pred = test_df[\"predicted\"]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n Metrics for Evaluation\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-Class Performance:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    metrics_df = pd.DataFrame([metrics_dict])\n",
    "    metrics_df.to_csv(\"results/metrics/test_metrics.csv\", index=False)\n",
    "    print(\"\\nEvaluation metrics saved to 'results/metrics/test_metrics.csv'\")\n",
    "\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = (\n",
    "        precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    )\n",
    "    per_class_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": [\"Negative\", \"Neutral\", \"Positive\"],\n",
    "            \"Precision\": precision_per_class,\n",
    "            \"Recall\": recall_per_class,\n",
    "            \"F1-Score\": f1_per_class,\n",
    "            \"Support\": support_per_class,\n",
    "        }\n",
    "    )\n",
    "    per_class_df.to_csv(\"results/metrics/per_class_metrics.csv\", index=False)\n",
    "    print(\"Per-class metrics saved to 'results/metrics/per_class_metrics.csv'\")\n",
    "\n",
    "    plot_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    test_df.to_csv(\"results/metrics/test_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to 'results/metrics/test_predictions.csv'\")\n",
    "\n",
    "    errors = test_df[test_df[\"sentiment\"] != test_df[\"predicted\"]]\n",
    "    error_rate = len(errors) * 100 / len(test_df)\n",
    "    print(f\"Error percentage: {error_rate:.2%}\")\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        errors.to_csv(\"results/metrics/error_examples.csv\", index=False)\n",
    "        print(\"Errors saved to 'results/metrics/error_examples.csv'\")\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok= True)\n",
    "os.makedirs(\"results/metrics\", exist_ok = True)\n",
    "\n",
    "train, val, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
